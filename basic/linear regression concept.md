**Linear Regression**

X=[1,2,3]일때 Y=[3,5,7]이라고 하면 그럼 X=4일때 Y는? 

=> f(x)=2x+1이라서 9일 것이다!

일반적으로 사람들은 이렇게 계산을 할 것이다.  그리고 그 알맞은 f(x)를 찾는 것이 **선형 회귀(Linear Regression)**이다.

다시 위의 f(x)식을 H(w,b)=Wx+b로 써보자. 우리의 목표는 W=2, b=1을 만드는 것이다.

가설을 초기화해서, w=1이고 b=0이라고 가정을 하면, **원래의 그래프(원래의 값)와 얼마나 차이가 나는지를 cost(W,b)**라고 하고, 보통 그 차이를 최소제곱법을 이용한다. 왜냐하면 잘못예측했을때 큰값을 발생시켜 오차 범위를 줄이고, 절대값을 이용한다면 연산속도가 많이 들기 때문이다.

따라서 **cost(W,b)=1/m*∑(Wxi+b-yi)^2(i=1~m)**라고 정의한다. 이때 m은 주어진 데이터의 갯수이고, Wxi+b는 예측한 값, yi는 실제값이다. 

결국, 우리가 궁극적으로 해결해야 할 문제는 cost의 값을 줄여  W=2,b=1을 도출해 내는 것이다. (=**min∑(Wxi+b-yi)^2(i=1~m)** )

이제 cost(W,b)의 값을 줄이는 과정이다. 우선 b를 고정하고 W(기울기)만 변경한다고 가정하면 W에 대한 cost의 그래프는 2차함수가 되고, 가장 낮은 부분이 global optimum(=2)가 될 것 이다. 이때 2차함수의 접선의 기울기로 W를 계속해서 찾아나가는데, 기울기가 음수라면 오른쪽으로 이동 (+), 기울기가 양수라면 왼쪽으로 이동(-)을 할 것 이다. 이를 **경사하강법**이라고하고, 다시 설명하자면 **cost를 줄이기 위해 반복적으로 기울기를 계산하여 변수의 값을 변경하는 과정**이다. W를 고정하고 b(절편)만 변경한다고 가정했을때도 b가 global optimum(=1)이 되는 곳을 찾아야 한다.

이때 W에대한 cost의 값에서 접선의 기울기를 구하는 방법은, cost(W,b)를 w에 대해 미분한 값을 구하고 ,b에 대한 cost의 값에서 접선의 기울기를 구하는 방법은 cost(W,b)를 b에대해 미분한 값을 구하는 것이다.

결국 머신러닝은 경사하강법을 통해 이루어지고, 어떤 data가 input값으로 들어왔을때, 그에 알맞은 선형 방정식 H(W,b)를 도출해 내야 하는데, 이때 cost(W,b)를 계속 줄이면서 알맞은 선형방정식을 도출하는 과정으로 이루어진다. cost(W,b)가 0이 되면 종료하고, 아니라면 각 W와 b를 미분한값을 빼주어 update하는 과정을 거쳐 epoch를 1000회이상 실행해야 한다.

